<!DOCTYPE HTML>
<!--
	Template Provided By:
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Final EAS 510 Project - Qiuxing Chen </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Final Movie Genre Project</a></li>
							<li><a href="#one">Introduction & Objectives</a></li>
							<li><a href="#two">Dataset Description</a></li>
							<li><a href="#three">Methodology (Text)</a></li>
							<li><a href="#four">Results & Evaluation</a></li>
							<li><a href="#five">Reproducibility & Instructions</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<h1>Final EAS 510 Project - Qiuxing Chen</h1>
							<p>This is a fine responsive site template that I will use for my project, designed by <a href="http://html5up.net">HTML5 UP</a><br />
							and released for free under the <a href="http://html5up.net/license">Creative Commons</a>.</p>

							<p>I am an Engineering Education PhD student, and do not have any Data Science and Computer Science background so a lot of this was very new to me (and I gave my best attempt at this with the time and knowledge I had). My group project partner also dropped out of the class, so there is no vision aspect of this project. I tried my best with the Data gathering & exploration, though it is still unbalanced. The NLP text models I implemented are RNN and BERT (uncased). I referenced many Medium.com articles and other documentation for this (including PyTorch and Pandas). I also referenced the class notes that are posted on the course website for batching and training loops.
							</p>
							
						</div>
					</section>

				<!-- One -->
					<section id="one" class="wrapper style2 spotlights">
						<section>
							<div class="content">
								<div class="inner">
									<h2>Introduction & Objectives</h2>
									<p>	The goal of the multi-modal movie genre classifier is to identify movies for specified genres. Most movies have multiple genres, so it would be useful to apply muli-label classification on plot summaries/overviews. This project only has the NLP (text-implementation) as my group partner was responsible for the visionmodel aspect of the project. The objectives in this project include: </p>
									<ul>
										<li>Data Handling & Pre-processing</li>
										<li>Modeling Approaches (Text)</li>
										<li>Evaluation</li>
										<li>Optimization</li>
									</ul>
								</div>
							</div>
						</section>

				<!-- Two -->
					<section id="two" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Dataset Description</h2>
							<p>The original dataset is from "Letterboxd (Movies Dataset)" by Simon Garanin from Kaggle.com and is sourced from GPL3 (https://www.gnu.org/licenses/gpl-3.0.en.html). As I am deploying this via GitHub Pages, it counts as distribution. The amount of data cleaned up (removing N/A, fill out genres, etc.) amounts to 27068 movies. The distribution of the original dataset is seen below: </p>
							<img src="images/Distribution-of-Movie-Genres.png" alt="Movie Genres", width= 1000, height = 1000>

							<p>Supplemental data is from TMDB (as specified by TMDB API Attribution page, the logo is included below for the use of their data). </p>
							<img src="images/TMDB-Credit-Logo.png" alt="TMDB Credit", width= 300, height = 300>

							<div class="features">
								<section>
									<span class="icon solid major fa-code"></span>
									<h3>Data Distribution (Genres)</h3>
									<p>The genres in the dataset are: Action, Comedy, Adventure, Thriller, Drama, Science Fiction, Music, Romance, History, Crime, Animation, Mystery, Horror, Fantasy, War, Western, TV Movie, Documentary.</p>
								</section>
								<section>
									<span class="icon solid major fa-lock"></span>
									<h3>Unbalanced Data</h3>
									<p>Observation of the distribution of datasets show that the data is highly unbalanced, with the most prevalent genres being Drama & Comedy. The least represented genres are Western, TV Movie, War, History, Music, and Family. I attempted to correct this by using supplemental data from TMDB on the specified underrepresented genres. The Gaussian distribution of the original dataset is shown below: </p>
									<img src="images/original_gauss.png" alt="Original Dataset Gaussian Distribution", width= 500, height = 500>

									<p>The cleaned csv's (genre and movies) are then uploaded to a Kaggle account for me to pull the data using Kaggle's API. The id's in both csv's are utilized to create a movie dictionary where the ID is matched to the genres. The values in the dictionary were used to create a co-occurence matrix. For movies that had more than two genres, I used itertool's permutations and python frozenset and looped the dictionary to remove any duplicates (for example, "Action, Comedy" and "Comedy, Action" are duplicates). The values are converted into a pandas dataframe and plotted with Seaborn Heatmap. </p>
								</section>
								<section>
									<span class="icon solid major fa-cog"></span>
									<h3>Co-Occurrence Matrix</h3>
									<p>Pictured below is the co-occurrence matrix for the movie genres. By inspection, the highest combinations are with (Comedy, Drama), (Drama, Romance), and (Drama, Thriller). The lowest combinations are (Documentary, Western), (Documentary, Thriller), and (History, Science Fiction).</p>
									<img src="images/Co-Occurrence-Matrix-of-Movie-Genres.png" alt="Co-Occurrence Matrix", width= 1000, height = 1000>
								</section>
								<section>
									<span class="icon solid major fa-link"></span>
									<h3>Dataset Balancing</h3>
									<p>After gathering the data, I attempted to balance the data in two ways. The first was simply finding more data (done through TMDB) and appended it to the existing Pandas dataframe that I had with the data from the original dataset (data increase of 400). This has only slightly increased the presence of the underrepresented genres, as shown in the barplot and Gaussian distribution below: </p>
									<img src="images/Distribution-of-Movie-Genres-(with-more-data).png" alt="More Data Distribution of Movies", width= 1000, height = 1000>
									<img src="images/second_gauss.png" alt="Second Dataset Gaussian Distribution", width= 500, height = 500>
									<p>This did not fully balance the data, so I applied random undersampling to the overrepresented data and upsampling to the underrepresented data as recommended by recent sources such as Medium.com and Google for Developers</p>
									<ul class="actions">
								<li><a href="https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets" class="button">Google for Developers Machine Learning Datasets: Imbalanced Datasets</a></li>
								<li><a href="https://medium.com/@tam.tamanna18/handling-imbalanced-datasets-in-python-methods-and-procedures-7376f99794de" class="button">Medium.com Handling Imbalanced Datasets in Python: Methods and Procedures</a></li>
							</ul>
								</section>
								<section>
									<span class="icon major fa-gem"></span>
									<h3>Text Preprocessing</h3>
									<p>The data is in the form of a pandas dataframe and has columns "description" and "genres". These genre labels are passed through Scikit's MultiLabel Binarizer and trainsformed, which converted them into a Numpy array of 1's and 0's. Some minor dataframe editing was done in order to ensure that the columns were "descriptions" and "genres" (or the category of genres) and not the index.</p>
									<p>NLTK packages "punkt", "stopwords", and "punkt_tab" were called to remove stopwords such as "the", "a", etc. that did not contribute much to understanding the text (compared to keywords).</p>
								</section>
							</div>
							
						</div>
					</section>

				<!-- Three -->
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Methodology (Text)</h2>
							<h3>Text Pre-processing</h3>
							<p>I spent a significant amount of time on this section in the beginning of the semester and used Bag of Words and passing the data through NLTK loops to remove unnecessary details like stop words, numbers and symbols. This method turned out to be incorrect (as noted in the presentation in April) and I restarted the whole process from scratch with Tokenizer and BERT Tokenizer.</p>
							<h4>Tokenization</h4>
							<p>Tokenization is converting the text into smaller sequences and parts (known as tokens). For example, a text "A brown fox jumped over a bush" will be converted into ["A", "brown", "fox", "jumped", "over", "a", "bush"] and I passed my descriptions into Tensorflow keras Tokenizer for the RNN Model. I utilized post padding (adding zeros to the end of sequence) in order to make the sequences uniform in size. This is an important step as sequences of different sizes will lead to errors. Post padding is also preferred in transformer models.</p>
							<ul>
								<li><a href="https://www.tensorflow.org/guide/keras" class="button"> Keras Tensorflow </a></li>
								<li><a href="https://saadsohail5104.medium.com/understanding-padding-in-nlp-types-and-when-to-use-them-bacae6cae401#:~:text=Types%20of%20Padding%20in%20NLP&text=Pre%2DPadding%20(Default)%3A,the%20end%20of%20a%20sequence." class="button"> Medium.com Understanding padding in NLP </a></li>
							</ul>

							<p>In the RNN Model, I referenced the PyTorch article "NLP From Scratch: Classifying Names with a Character-Level RNN" by Sean Robertson and modified his model with embedded dimensions and batching. Sigmoid activation functions were included in the class for non-linearity and is useful in this case as the genre labels were binarized using Multilabel Binarizer. The outputs of this sigmoid activation function would fall between 0 and 1. If I had not done so, a softmax activation function would be more appropriate as it would be a multi-class classification problem. For the embedding dimension, torch.nn.Embedding was utilized to store the word embeddings and indices. A linear activation function was applied to the hidden and output sizes (self.h2o) in combination with the non-linear activation function in order to enhance the model's learning and prediction. This is the hidden layer activation. Finally, the processed data is split into batches and split using PyTorch's Dataloader and Scikit's train_test_split functions. I set a small batch size of 5 and performed an 80/20 train/test split. I originally set the batch to 16, but it was too big and giving me errors when I ran it.</p>

							<p>For BERT, I utilized the BERT Tokenizer that came with the HuggingFace BERT uncased transformer package. I used truncation to remove elements that exceed the length of the specified maximum length (which was based on the description length). Attention masks were employed using BERT's encoder plus function. The IDs, attention masks, and labels were then batched for training and testing.</p>
							<ul>
								<li><a href="https://medium.com/@piyushkashyap045/guide-to-tokenization-and-padding-with-bert-transforming-text-into-machine-readable-data-5a24bf59d36b" class="button"> Medium.com Guide to Tokenization and Padding with BERT: Transforming Text into Machine-Readable Data </a></li>
							</ul>
							<ul>
								<li><a href="https://pytorch.org/docs/stable/data.html" class="button">PyTorch DataLoader</a></li>
								<li>Class notes on batching and splitting data in "PyTorch Computer Vision" were also used</li>
							</ul>
							<h3>Hyperparameters</h3>
							<h4>RNN Model</h4>
							<p>The following hyperparameters were used for the RNN model: </p>
							<ul>
								<li>Optimizer: AdamW</li>
								<li>Loss Function: BCEWithLogitsLoss</li>
								<li>Learning Rate: 1e-10</li>
								<li>Input Size: 39,564</li>
								<li>Output Size: 19 (# of Genre Types)</li>
								<li>Batch Size: 5</li>
							</ul>
							<h4>BERT Uncased Transformer Model</h4>
							<p>The following hyperparameters were used for the BERT uncased model: </p>
							<ul>
								<li>Optimizer: AdamW</li>
								<li>Loss Function: CrossEntropyLoss</li>
								<li><a href="https://medium.com/data-science/a-complete-guide-to-bert-with-code-9f87602e4a11" class="button">Scheduler Steps (for variable learning rate) were used</a></li>
								<li><a href="https://arxiv.org/pdf/1810.04805" class="button">BERT Paper</a></li>
								<li><a href="https://huggingface.co/google-bert/bert-base-uncased"class="button">BERT HuggingFace Model Documentation</a></li>
							</ul>
						</div>
					</section>

				<!-- Four -->
					<section id="four" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Results & Evaluation</h2>
							<p>F1 scores were used in each Epoch to measure the performance of the model. It takes into consideration the precision and recall scores and returns an evaluation score. This is particularly helpful when the data is imbalanced as it takes into account a harmonic mean rather than an average. I flattened the values to remove the error that F1 score gave when running the initial testing. This was necessary since this was a multi-label classification problem and the model is trying to predict multiple labels rather than just a single value.</p>

							<p>I attempted to create confusion matrices, but they returned two 2x2 matrix results rather than all the classes when using Scikit multilabel_confusion_matrix so it is omitted here (no visual representation).</p>
							<div class="features">
								<section>
									<span class="icon solid major fa-code"></span>
									<h3>Training & Testing Loss vs Epoch (RNN)</h3>
									<img src="images/Baseline-Training-Testing-Loss-vs-Epoch.png" alt="RNN test train loss", width= 500, height = 500>
									<p>The RNN model ran for 50 epochs and I recorded the average loss (with batches) for each batch to plot and output. This model took approximately 15 minutes to run the train and test loops.</p>
									<img src="images/train-test-rnn.png" alt="Train-test results RNN", width= 500, height = 500>
								</section>
								<section>
									<span class="icon solid major fa-lock"></span>
									<h3>F1 Score vs Epoch (RNN)</h3>
									<p>The F1 score for the RNN model stayed consistent around 50% which meant that the model wasn't doing well and was making random guesses.</p>
									<img src="images/F1-Score-vs-Epoch.png" alt="RNN F1 score", width= 500, height = 500>
								</section>
								<section>
									<span class="icon solid major fa-cog"></span>
									<h3>Training & Testing Loss vs Epoch (BERT)</h3>
									<img src="images/BERT-Training-Testing-Loss-vs-Epoch.png" alt="BERT test train loss", width= 500, height = 500>
									<p>BERT trained for 4 epochs (the BERT article recommended 2-4 epochs for fine-tuning since it is already a pre-trained model). As with the RNN Model, an average training and testing loss were applied to the loop to record loss at each batch. </p>
									<img src="images/BERT-train-test-rnn.png" alt="Train-test results BERT", width= 500, height = 500>
								</section>
								<section>
									<span class="icon solid major fa-desktop"></span>
									<h3>F1 Score vs Epoch (BERT)</h3>
									<p>The F1 score for the BERT model was poor (though it was better than the 45.7% for RNN) and reported 51.8%.</p>
									<img src="images/BERT-F1-Score-vs-Epoch.png" alt="BERT F1 score", width= 500, height = 500>
								</section>
								
							</div>
						</div>
					</section>
				<!-- Five -->
					<section id="five" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Reproducibility & Instructions</h2>
							<p>To reproduce the model, please utilize Google Colab and a GPU for a smoother and faster experience. I started this project on Jupyter Notebooks and it gets slow and laggy given the dataset is so large and the models are computationally intensive. Utilize the code that is provided labeled "Final EAS 510 Project.ipynb" (a notebook file) and it should run automatically with the outlined steps (with the exception of adding a kaggle.json file for the API).</p>
							<div class="features">
								<section>
									<span class="icon solid major fa-code"></span>
									<h3>Importing Relevant Libraries</h3>
									<p>To get started, first import the relevant libraries, including PyTorch and Scikit. It will also be necessary to have a Kaggle account and kaggle hub installed.</p>
									<ul>
										<li>PyTorch (torch): torch.nn, torch.utils.data (Dataset, Dataloader), torch.nn.utils.rnn (pad_sequence)</li>
										<li>Scikit (sklearn): sklearn.preprocessing (MultiLabelBinarizer), sklearn.utils (resample), sklearn.metrics (recall_score, accuracy score, f1_score)</li>
										<li>Pandas (pd) for dataframe work</li>
										<li>Numpy (np) for conversion into arrays</li>
										<li>zipfile (ZipFile) for reading zipped files (that contain the dataset CSVs for the project)</li>
										<li>Plotting libraries such as Seaborn (sns) and matplotlib (plt)</li>
										<li>Google.colab import files (to upload kaggle.json API key)</li>
										<li>itertools (permutations)</li>
										<li>tensorflow (tf): tensorflow.keras, tensorflow.keras.models (Sequential), tensorflow.keras.layers (LSTM, Dense, Embedding), tensorflow.keras.preprocessing.text (Tokenizer), tensorflow.keras.preprocessing.sequence (pad_sequences)</li>
									</ul>
								</section>
								<section>
									<span class="icon solid major fa-lock"></span>
									<h3>Upload kaggle.json & Importing the dataset</h3>
									<p>Upload the kaggle.json that you can get from your Kaggle API in your account. This will allow you to access the dataset using kaggle hub. For the dataset that I modified, I uploaded the file onto kaggle, and it can be obtained with "!kaggle datasets download -d 510-project-letterboxd-cleaned"</p>
								</section>
								<section>
									<span class="icon solid major fa-cog"></span>
									<h3>Creating the File Paths</h3>
									<p>It is necessary to unzip the zipfile and store the files in a local folder. In this case, the folder created was named "Extracted_Files" and paths were copied from the folder to link them to variable names. These names were then passed on to pandas in order to create dataframes from the extracted csv files.</p>
								</section>
								<section>
									<span class="icon solid major fa-desktop"></span>
									<h3>Movie IDs, dictionary, and data processing</h3>
									<p>The extracted files are now linked to two separate dataframes. The genres csv has significantly more entries due to the genres being spread across different rows (for example, ID 1000001 has entries in the first and second row with genres "Comedy" and "Adventure"). I combined the two dataframes together so that each row has the unique ID, movie description, name, date, tagline, minute, rating, and genres. There are more information provided than what is needed for the purpose of the project, so I retained only "description" and "genres".</p>

									<p>A movie dictionary was created through iterating each row of the genre dataframe. It returns a dictionary where the keys are the movie IDs and the values are the genres. For example: {'1000001': ['Comedy', 'Adventure'], '1000002': ['Comedy', 'Thriller', 'Drama'], '1000003': ['Science Fiction', 'Adventure', 'Comedy', 'Action'], '1000004': ['Drama'], '1000005': ['Drama', 'Comedy', 'Music', 'Romance'] ...}</p>

									<p>These key value pairs are helpful for creating a co-occurence matrix. For values that had more than a length of 2, permutations will be applied using itertools permutations. These permutations are then evaluated for duplicates and any values that fit the criteria are removed using python frozenset. For example, permutations "Action, Comedy" and "Comedy, Action" are considered duplicates. These values can be passed on to pandas dataframe to create a co-occurrence matrix. The dataframe is also used to merge with the additional data (as they have the same format and columns).</p>
								</section>
								<section>
									<span class="icon solid major fa-link"></span>
									<h3>Tokenizer (RNN)</h3>
									<p>For the RNN model, import TensorFlow's Keras library (as listed above). Tokenize the descriptions column and pad the sequences so they are all of the same length. This is important as different descriptions will have different lengths, which could cause potential errors. Pass the tokenized data into a with scikit train_test_split.</p>

									<h3>Tokenizer (BERT)</h3>
									<p>For BERT, utilize the pre-trained model's tokenizer by passing the descriptions column through BertTokenizer.pretrained('bert-base-uncased') through its encoder_plus function. BERT uncased is used as it is not strict on the case of the letters (capital vs lowercase), but in the preprocessing of the code, all the descriptions are converted into lowercase just in case. Pass the tokenized text, token_ids, attention_masks and labels into a train_test_split.</p>
								</section>
								<section>
									<span class="icon major fa-gem"></span>
									<h3>Training Loop</h3>
									<p>Please refer to the hyperparameters specified above for each model. Be sure to set the model to training mode using model.train() and to pass the data into the device being used (either cuda or cpu). In the training loop, the data will:</p>
									<ul>
										<li>Be passed onto the model for training and converted into logits</li>
										<li>The logits are passed through a sigmoid activation function</li>
										<li>Loss is calculated and the weights and biases are optimized through the optimizer</li>
										<li>Backpropogation occurs and the optimizer takes a step forward</li>
									</ul>

									<h3>Testing Loop</h3>
									<p>Set the model into evaluation mode by passing the model through model.eval() and using inference mode from PyTorch. It is similar to the training process, except now testing values are being used with the logits gained from the training loop stage. F1 scores are calulated using scikit fl_score.</p>
								</section>
							</div>

							<section>
								<h2>References used for this project</h2>
								<h3>Papers</h3>
							<ul class="actions">
								<li><a href="https://arxiv.org/pdf/1810.04805" class="button">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
								<li><a href="https://arxiv.org/pdf/1706.03762" class="button">Attention Is All You Need</a></li>
								<li><a href="https://arxiv.org/pdf/1905.11946" class="button">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></li>
							</ul>
								<h3>Online References (including course notes) </h3>
								<ul class="actions">
								<li><a href="https://csragtoriches.com/teaching/aibasic/" class="button">EAS 510 Course Notes</a></li>
								<li><a href="https://pytorch.org/docs/stable/index.html" class="button">PyTorch Documentation Library</a></li>
								<li><a href="https://stackoverflow.com/questions/6740918/creating-a-dictionary-from-a-csv-file" class="button">stack overflow: Creating a dictionary from a csv file</a></li>
								<li><a href="https://docs.vultr.com/python/built-in/frozenset" class="button">Vultr - Python frozenset()</a></li>
								<li><a href="https://docs.python.org/3/library/collections.html#collections.defaultdict" class="button">Python defaultdict()</a></li>
								<li><a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html" class="button">Pandas DataFrame documentation</a></li>
								<li><a href="http://stackoverflow.com/questions/65324814/how-could-i-count-all-the-genres-in-my-dataframe" class="button">stack overflow: counting genres in dataframe</a></li>
								<li><a href="https://stackoverflow.com/questions/29314033/drop-rows-containing-empty-cells-from-a-pandas-dataframe" class="button">stack overflow: dropping empty cells from dataframe</a></li>
								<li><a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html" class="button">Pandas DataFrame groupby() documentation</a></li>
								<li><a href="https://seaborn.pydata.org/generated/seaborn.histplot.html" class="button">Seaborn Histoplot documentation</a></li>
								<li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html" class="button">scipy Gaussian Distribution documentation</a></li>
								<li><a href="https://medium.com/@tam.tamanna18/handling-imbalanced-datasets-in-python-methods-and-procedures-7376f99794de" class="button">Medium.com Handling imbalanced datasets in Python Methods and Procedures</a></li>
								<li><a href="https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets" class="button">Google for Developers Machine Learning Datasets: Imbalanced Datasets</a></li>
								<li><a href="https://www.nltk.org/api/nltk.tokenize.word_tokenize.html" class="button">NLTK word tokenizing documentation</a></li>
								<li><a href="https://sharmasaravanan.medium.com/text-generation-using-lstm-a-step-by-step-guide-9b787467f9de" class="button">Medium.com Text Generation using LSTM Guide</a></li>
								<li><a href="https://saadsohail5104.medium.com/understanding-padding-in-nlp-types-and-when-to-use-them-bacae6cae401" class="button">Medium.com Padding in NLP Types</a></li>
								<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index" class="button">Tensorflow keras word_index documentation</a></li>
								<li><a href="https://www.geeksforgeeks.org/implementing-recurrent-neural-networks-in-pytorch/" class="button">GeeksforGeeks RNN in PyTorch</a></li>
								<li><a href="https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial" class="button">PyTorch Character Level RNN Classification Tutorial documentation</a></li>
								<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" class="button">PyTorch Embedding Documentation</a></li>
								<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" class="button">Scikit F1-Score documentation</a></li>
								<li><a href="stackoverflow.com/questions/73640393/precision-recall-f1-score-with-sklearn-on-pytorch" class="button">stack overflow: Precision recall F1 score with sklearn on PyTorch</a></li>
								<li><a href="https://huggingface.co/google-bert/bert-base-uncased" class="button">HuggingFace BERT uncased transformer model</a></li>
								<li><a href="https://medium.com/data-science/a-complete-guide-to-bert-with-code-9f87602e4a11" class="button">Medium.com A complete guide to BERT with code</a></li>
								<li><a href="https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/10" class="button">Multi-label Classification in PyTorch</a></li>
								<li><a href="https://ublearns.buffalo.edu" class="button">UBLearns course notes and assignments</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Template Copyrights. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>